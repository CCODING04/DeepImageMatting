{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:31.557310Z",
     "start_time": "2018-11-10T02:28:31.549086Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:33.188706Z",
     "start_time": "2018-11-10T02:28:31.765577Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:33.194539Z",
     "start_time": "2018-11-10T02:28:33.191175Z"
    }
   },
   "outputs": [],
   "source": [
    "MEAN_RGB = np.array([123.68, 116.779, 103.939], dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:34.164516Z",
     "start_time": "2018-11-10T02:28:33.196534Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dict = np.load('./PretrainedModel/vgg16.npy',encoding='latin1').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:34.176588Z",
     "start_time": "2018-11-10T02:28:34.167934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv1_1',\n",
       " 'conv1_2',\n",
       " 'conv2_1',\n",
       " 'conv2_2',\n",
       " 'conv3_1',\n",
       " 'conv3_2',\n",
       " 'conv3_3',\n",
       " 'conv4_1',\n",
       " 'conv4_2',\n",
       " 'conv4_3',\n",
       " 'conv5_1',\n",
       " 'conv5_2',\n",
       " 'conv5_3',\n",
       " 'fc6',\n",
       " 'fc7',\n",
       " 'fc8']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([i for i in data_dict.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:34.187821Z",
     "start_time": "2018-11-10T02:28:34.179893Z"
    }
   },
   "outputs": [],
   "source": [
    "layers = ['conv1_1',\n",
    "         'conv1_2',\n",
    "         'conv2_1',\n",
    "         'conv2_2',\n",
    "         'conv3_1',\n",
    "         'conv3_2',\n",
    "         'conv3_3',\n",
    "         'conv4_1',\n",
    "         'conv4_2',\n",
    "         'conv4_3',\n",
    "         'conv5_1',\n",
    "         'conv5_2',\n",
    "         'conv5_3',\n",
    "         'fc6',\n",
    "         'fc7',\n",
    "         'fc8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:34.199446Z",
     "start_time": "2018-11-10T02:28:34.191090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer conv1_1, \t\t Weight shape: (3, 3, 3, 64), \t\tBias shape (64,)\n",
      "Layer conv1_2, \t\t Weight shape: (3, 3, 64, 64), \t\tBias shape (64,)\n",
      "Layer conv2_1, \t\t Weight shape: (3, 3, 64, 128), \t\tBias shape (128,)\n",
      "Layer conv2_2, \t\t Weight shape: (3, 3, 128, 128), \t\tBias shape (128,)\n",
      "Layer conv3_1, \t\t Weight shape: (3, 3, 128, 256), \t\tBias shape (256,)\n",
      "Layer conv3_2, \t\t Weight shape: (3, 3, 256, 256), \t\tBias shape (256,)\n",
      "Layer conv3_3, \t\t Weight shape: (3, 3, 256, 256), \t\tBias shape (256,)\n",
      "Layer conv4_1, \t\t Weight shape: (3, 3, 256, 512), \t\tBias shape (512,)\n",
      "Layer conv4_2, \t\t Weight shape: (3, 3, 512, 512), \t\tBias shape (512,)\n",
      "Layer conv4_3, \t\t Weight shape: (3, 3, 512, 512), \t\tBias shape (512,)\n",
      "Layer conv5_1, \t\t Weight shape: (3, 3, 512, 512), \t\tBias shape (512,)\n",
      "Layer conv5_2, \t\t Weight shape: (3, 3, 512, 512), \t\tBias shape (512,)\n",
      "Layer conv5_3, \t\t Weight shape: (3, 3, 512, 512), \t\tBias shape (512,)\n",
      "Layer fc6, \t\t Weight shape: (25088, 4096), \t\tBias shape (4096,)\n",
      "Layer fc7, \t\t Weight shape: (4096, 4096), \t\tBias shape (4096,)\n",
      "Layer fc8, \t\t Weight shape: (4096, 1000), \t\tBias shape (1000,)\n"
     ]
    }
   ],
   "source": [
    "# 打印出每一层的shape信息\n",
    "for k in layers:\n",
    "    w = data_dict[k][0]\n",
    "    b = data_dict[k][1]\n",
    "    print('Layer {0}, \\t\\t Weight shape: {1}, \\t\\tBias shape {2}'.format(k, w.shape, b.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:34.352346Z",
     "start_time": "2018-11-10T02:28:34.347216Z"
    }
   },
   "outputs": [],
   "source": [
    "global wd, num_classes\n",
    "num_classes = 3\n",
    "wd = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:37.426476Z",
     "start_time": "2018-11-10T02:28:37.419504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([768, 576])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([4,3])*32*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:37.830058Z",
     "start_time": "2018-11-10T02:28:37.824814Z"
    }
   },
   "outputs": [],
   "source": [
    "# resize scale = (576, 768)\n",
    "# img shape = (768, 576)\n",
    "resize_scale = (576, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:38.354029Z",
     "start_time": "2018-11-10T02:28:38.328946Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(dtype=tf.float32, shape = (None, 768, 576, 4), name= 'inputs')\n",
    "labels = tf.placeholder(dtype=tf.int32, shape = (None, 768, 576, 3), name = 'labels')\n",
    "keeprob = tf.placeholder(dtype=tf.float32, shape = (), name = 'keeprob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:40.177017Z",
     "start_time": "2018-11-10T02:28:40.168391Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_conv_filter(name):\n",
    "    # filter shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    weight = data_dict[name][0]\n",
    "    if name == 'conv1_1':\n",
    "        weight_mean = weight.mean( axis = 2)\n",
    "        in_channels = weight.shape[2]\n",
    "        tmp = [weight[:,:,i,:] for i in range(in_channels)]\n",
    "        tmp.append(weight_mean)\n",
    "        weight = np.stack( tmp, axis = 2 )\n",
    "    #print(weight.shape)\n",
    "    init = tf.constant_initializer(value=weight, dtype=tf.float32)\n",
    "    shape = weight.shape\n",
    "    var = tf.get_variable(name=\"filter\", initializer=init, shape=shape)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:40.673164Z",
     "start_time": "2018-11-10T02:28:40.664248Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bias(name):\n",
    "    bias_weights = data_dict[name][1]\n",
    "    shape = bias_weights.shape\n",
    "    if name == 'fc8':\n",
    "        #tf.set_random_seed(1) \n",
    "        init = tf.contrib.layers.xavier_initializer(seed = 1)\n",
    "        shape = [num_classes]\n",
    "    else:\n",
    "        init = tf.constant_initializer(value=bias_weights, dtype=tf.float32)\n",
    "    #print('I am ok')\n",
    "    var = tf.get_variable(name=\"biases\", shape=shape, initializer=init)\n",
    "    return var  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:41.110879Z",
     "start_time": "2018-11-10T02:28:41.105101Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_layer( x, name):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        filt = get_conv_filter(name)\n",
    "        bias = get_bias(name)\n",
    "\n",
    "        x = tf.nn.conv2d(x, filt, [1, 1, 1, 1], padding='SAME')\n",
    "        x = tf.nn.bias_add( x, bias)\n",
    "        x = tf.nn.relu(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:41.572519Z",
     "start_time": "2018-11-10T02:28:41.568232Z"
    }
   },
   "outputs": [],
   "source": [
    "def max_pool(x, name):\n",
    "    return tf.nn.max_pool(x , ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:42.100452Z",
     "start_time": "2018-11-10T02:28:42.091970Z"
    }
   },
   "outputs": [],
   "source": [
    "def fc_layer( x, name, num_classes = num_classes, relu = True):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        # shape = x.get_shape().as_list()\n",
    "        if name == 'fc6':\n",
    "            shape = [7, 7, 512, 4096]\n",
    "        elif name == 'fc7':\n",
    "            shape = [1, 1, 4096, 4096]\n",
    "        elif name == 'fc8':\n",
    "            shape = [1, 1, 4096, num_classes]\n",
    "        init = tf.contrib.layers.xavier_initializer( seed = 1, dtype = tf.float32)\n",
    "        filt = tf.get_variable( name = 'weights', initializer=init, shape = shape )\n",
    "        bias = get_bias(name)\n",
    "        \n",
    "        x = tf.nn.conv2d( x, filt, [ 1, 1, 1, 1], padding='SAME' )\n",
    "        x = tf.nn.bias_add( x, bias)\n",
    "        \n",
    "        if relu:\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:47.810733Z",
     "start_time": "2018-11-10T02:28:42.577141Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('encoder') as scope:\n",
    "    \n",
    "    ####conv1\n",
    "    x = inputs\n",
    "    x = conv_layer( x, 'conv1_1')\n",
    "    x = conv_layer( x, 'conv1_2')\n",
    "    pool1_out = max_pool(x, 'pool1')\n",
    "    \n",
    "    ####conv2\n",
    "    x = conv_layer( pool1_out, 'conv2_1')\n",
    "    x = conv_layer( x, 'conv2_2')\n",
    "    pool2_out = max_pool(x, 'pool2')\n",
    "    \n",
    "    ####conv3\n",
    "    x = conv_layer( pool2_out, 'conv3_1')\n",
    "    x = conv_layer( x, 'conv3_2')\n",
    "    x = conv_layer( x, 'conv3_3')\n",
    "    pool3_out = max_pool( x, 'pool3')\n",
    "    \n",
    "    ####conv4\n",
    "    x = conv_layer( pool3_out, 'conv4_1')\n",
    "    x = conv_layer( x, 'conv4_2')\n",
    "    x = conv_layer( x, 'conv4_3')\n",
    "    pool4_out = max_pool( x, 'pool4')\n",
    "    \n",
    "    ####conv5\n",
    "    x = conv_layer( pool4_out, 'conv5_1')\n",
    "    x = conv_layer( x, 'conv5_2')\n",
    "    x = conv_layer( x, 'conv5_3')\n",
    "    pool5_out = max_pool( x, 'pool5')\n",
    "    \n",
    "    ####fc6\n",
    "    x = fc_layer( pool5_out, 'fc6')\n",
    "    x = tf.nn.dropout( x, keep_prob=keeprob)\n",
    "    \n",
    "    ####fc7\n",
    "    fc7_out = fc_layer( x, 'fc7')\n",
    "    #x = tf.nn.dropout( fc7_out, keep_prob=keeprob)\n",
    "    # shape (None, 20, 20, 4096)\n",
    "    \n",
    "    ####fc8\n",
    "    #x = fc_layer( x, 'fc8')\n",
    "    #x = tf.argmax( x, axis = 3)\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:47.816607Z",
     "start_time": "2018-11-10T02:28:47.813285Z"
    }
   },
   "outputs": [],
   "source": [
    "stddev_1x1 = 0.001\n",
    "stddev_conv2d_trans = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:47.831460Z",
     "start_time": "2018-11-10T02:28:47.818785Z"
    }
   },
   "outputs": [],
   "source": [
    "l2_regularization_rate = tf.placeholder(dtype=tf.float32, shape=[], name='l2_regularization_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:48.001054Z",
     "start_time": "2018-11-10T02:28:47.834535Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('decoder') as scope:\n",
    "    pool3_out_scaled = tf.multiply( pool3_out, 0.0001, name = 'pool3_out_scaled')\n",
    "    \n",
    "    pool3_1x1 = tf.layers.conv2d( inputs = pool3_out_scaled,\n",
    "                                  filters = num_classes,\n",
    "                                  kernel_size = ( 1, 1),\n",
    "                                  strides = (1, 1), \n",
    "                                  padding = 'same',\n",
    "                                  kernel_initializer = tf.truncated_normal_initializer( stddev=stddev_1x1),\n",
    "                                  kernel_regularizer = tf.contrib.layers.l2_regularizer(l2_regularization_rate),\n",
    "                                  name = 'pool3_1x1')\n",
    "    \n",
    "    pool4_out_scaled = tf.multiply( pool4_out, 0.01, name = 'pool4_out_scaled')\n",
    "    \n",
    "    pool4_1x1 = tf.layers.conv2d( inputs = pool4_out_scaled,\n",
    "                                  filters = num_classes,\n",
    "                                  kernel_size = (1,1),\n",
    "                                  strides = (1,1),\n",
    "                                  padding = 'same',\n",
    "                                  kernel_initializer = tf.truncated_normal_initializer( stddev=stddev_1x1),\n",
    "                                  kernel_regularizer = tf.contrib.layers.l2_regularizer(l2_regularization_rate),\n",
    "                                  name = 'pool4_1x1')\n",
    "    \n",
    "    fc7_1x1 = tf.layers.conv2d( inputs = fc7_out,\n",
    "                                filters = num_classes,\n",
    "                                kernel_size = (1,1),\n",
    "                                strides = (1,1),\n",
    "                                padding = 'same',\n",
    "                                kernel_initializer = tf.truncated_normal_initializer( stddev= stddev_1x1),\n",
    "                                kernel_regularizer = tf.contrib.layers.l2_regularizer( l2_regularization_rate),\n",
    "                                name = 'fc7_1x1')\n",
    "    \n",
    "    fc7_conv2d_trans = tf.layers.conv2d_transpose(inputs=fc7_1x1,\n",
    "                                                  filters=num_classes,\n",
    "                                                  kernel_size=(4, 4),\n",
    "                                                  strides=(2, 2),\n",
    "                                                  padding='same',\n",
    "                                                  kernel_initializer=tf.truncated_normal_initializer(stddev=stddev_conv2d_trans),\n",
    "                                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_regularization_rate),\n",
    "                                                  name='fc7_conv2d_trans')\n",
    "    \n",
    "    add_fc7_pool4 = tf.add(fc7_conv2d_trans, pool4_1x1, name='add_fc7_pool4')\n",
    "    \n",
    "    fc7_pool4_conv2d_trans = tf.layers.conv2d_transpose(inputs=add_fc7_pool4,\n",
    "                                                        filters=num_classes,\n",
    "                                                        kernel_size=(4, 4),\n",
    "                                                        strides=(2, 2),\n",
    "                                                        padding='same',\n",
    "                                                        kernel_initializer=tf.truncated_normal_initializer(stddev=stddev_conv2d_trans),\n",
    "                                                        kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_regularization_rate),\n",
    "                                                        name='fc7_pool4_conv2d_trans')\n",
    "    \n",
    "    add_fc7_pool4_pool3 = tf.add(fc7_pool4_conv2d_trans, pool3_1x1, name='add_fc7_pool4_pool3')\n",
    "    \n",
    "    fc7_pool4_pool3_conv2d_trans = tf.layers.conv2d_transpose(inputs=add_fc7_pool4_pool3,\n",
    "                                                              filters=num_classes,\n",
    "                                                              kernel_size=(16, 16),\n",
    "                                                              strides=(8, 8),\n",
    "                                                              padding='same',\n",
    "                                                              kernel_initializer=tf.truncated_normal_initializer(stddev=stddev_conv2d_trans),\n",
    "                                                              kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_regularization_rate),\n",
    "                                                              name='fc7_pool4_pool3_conv2d_trans')\n",
    "    \n",
    "    fcn8s_output = tf.identity(fc7_pool4_pool3_conv2d_trans, name='fcn8s_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:48.007978Z",
     "start_time": "2018-11-10T02:28:48.003711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(768), Dimension(576), Dimension(3)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcn8s_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:48.039957Z",
     "start_time": "2018-11-10T02:28:48.010053Z"
    }
   },
   "outputs": [],
   "source": [
    "from data_generator.batch_generator_v3 import BatchGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:28:48.209561Z",
     "start_time": "2018-11-10T02:28:48.205360Z"
    }
   },
   "outputs": [],
   "source": [
    "img_dir = './dataset/training/images/output/filterImg/'\n",
    "trimap_dir = './dataset/training/images/output/NewTrimap/'\n",
    "priorInfo_dir_lsit = ['./dataset/training/images/output/shapeMask/']\n",
    "alpha_dir = './dataset/training/images/output/filterAlpha/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:06.603704Z",
     "start_time": "2018-11-10T02:29:06.595339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbg = BatchGenerator(img_dirs = img_dir,\\n                    img_file_format = 'png',\\n                    prior_info_dir_list = priorInfo_dir_lsit,\\n                    ground_truth_dir = alpha_dir,\\n                    num_classes = 3)\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "bg = BatchGenerator(img_dirs = img_dir,\n",
    "                    img_file_format = 'png',\n",
    "                    prior_info_dir_list = priorInfo_dir_lsit,\n",
    "                    ground_truth_dir = alpha_dir,\n",
    "                    num_classes = 3)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:09.692871Z",
     "start_time": "2018-11-10T02:29:09.684366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvalid_inp, valid_gt = bg.genValidata(valid_size=200, \\n                                       convert_labels_to_one_hot=True)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "valid_inp, valid_gt = bg.genValidata(valid_size=200, \n",
    "                                       convert_labels_to_one_hot=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:09.878858Z",
     "start_time": "2018-11-10T02:29:09.874357Z"
    }
   },
   "outputs": [],
   "source": [
    "#print('valid input shape:',valid_inp.shape)\n",
    "#print('valid ground truth shape', valid_gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:10.276844Z",
     "start_time": "2018-11-10T02:29:10.271098Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:11.970036Z",
     "start_time": "2018-11-10T02:29:11.963277Z"
    }
   },
   "outputs": [],
   "source": [
    "from closed_form_matting import compute_laplacian\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:15.435110Z",
     "start_time": "2018-11-10T02:29:15.423796Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import handlers\n",
    "\n",
    "class Logger(object):\n",
    "    level_relations = {\n",
    "        'debug':logging.DEBUG,\n",
    "        'info':logging.INFO,\n",
    "        'warning':logging.WARNING,\n",
    "        'error':logging.ERROR,\n",
    "        'crit':logging.CRITICAL\n",
    "    }#日志级别关系映射\n",
    "\n",
    "    def __init__(self,filename,level='info',when='D',backCount=3,fmt='%(asctime)s - %(pathname)s[line:%(lineno)d] - %(levelname)s: %(message)s'):\n",
    "        self.logger = logging.getLogger(filename)\n",
    "        format_str = logging.Formatter(fmt)#设置日志格式\n",
    "        self.logger.setLevel(self.level_relations.get(level))#设置日志级别\n",
    "        sh = logging.StreamHandler()#往屏幕上输出\n",
    "        sh.setFormatter(format_str) #设置屏幕上显示的格式\n",
    "        th = handlers.TimedRotatingFileHandler(filename=filename,when=when,backupCount=backCount,encoding='utf-8')#往文件里写入#指定间隔时间自动生成文件的处理器\n",
    "        #实例化TimedRotatingFileHandler\n",
    "        #interval是时间间隔，backupCount是备份文件的个数，如果超过这个个数，就会自动删除，when是间隔的时间单位，单位有以下几种：\n",
    "        # S 秒\n",
    "        # M 分\n",
    "        # H 小时、\n",
    "        # D 天、\n",
    "        # W 每星期（interval==0时代表星期一）\n",
    "        # midnight 每天凌晨\n",
    "        th.setFormatter(format_str)#设置文件里写入的格式\n",
    "        self.logger.addHandler(sh) #把对象加到logger里\n",
    "        self.logger.addHandler(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:15.732085Z",
     "start_time": "2018-11-10T02:29:15.725996Z"
    }
   },
   "outputs": [],
   "source": [
    "log = Logger('./LOG/Time_Analysis.log',level='debug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:16.237659Z",
     "start_time": "2018-11-10T02:29:16.222657Z"
    }
   },
   "outputs": [],
   "source": [
    "def matting_laplacians_impl(xs, prob_bs, prob_fs):\n",
    "    #MEAN_BGR = np.array([104.00698793, 116.66876762, 122.67891434])\n",
    "    start_time = time()\n",
    "    def convert_sparse_matrix(X):\n",
    "        coo = X.tocoo()\n",
    "        indices = np.mat([coo.row, coo.col]).transpose()\n",
    "        return indices, coo.data, coo.shape\n",
    "    \n",
    "    xs = xs.copy()\n",
    "    prob_bs = prob_bs.copy()\n",
    "    prob_fs = prob_fs.copy()\n",
    "    MEAN_RGB = np.array([123.68, 116.779, 103.939], dtype = np.float32)\n",
    "    laplacians = list()\n",
    "    ret0 = list() # indices\n",
    "    ret1 = list() # value\n",
    "    ret2 = list() # dense_shape\n",
    "    for x, prob_b, prob_f in zip(xs, prob_bs, prob_fs):\n",
    "        img = x[:,:,:3]\n",
    "        img = (img + MEAN_RGB) / 255.0   # [-127:127] -> [0:1]\n",
    "\n",
    "        # Constant map  (Remove extra channel of (1, h, w))\n",
    "        consts_map = (0.9 < prob_b) | (0.9 < prob_f)\n",
    "        \n",
    "        laplacian = compute_laplacian(img, ~consts_map)\n",
    "        Xindices, Xdata, Xshape = convert_sparse_matrix(laplacian)\n",
    "        ret0.append(Xindices)\n",
    "        ret1.append(Xdata)\n",
    "        ret2.append(Xshape)\n",
    "        #laplacians.append(laplacian)\n",
    "    out0 = np.asarray(ret0)\n",
    "    out1 = np.asarray(ret1)\n",
    "    out2 = np.asarray(ret2)\n",
    "    cost_time = time() - start_time\n",
    "    \"\"\"\n",
    "    log.logger.debug('indices: {0} dtype:{1}'.format( (out0.shape), out0.dtype) )\n",
    "    log.logger.debug('values: {0} dtype:{1}'.format( (out1.shape), out1.dtype) )\n",
    "    log.logger.debug('dense_shape: {0} dtype:{1}'.format( (out2.shape), out2.dtype) )\n",
    "    \"\"\"\n",
    "    #log.logger.debug('Matting Laplacian Cost {0}'.format(cost_time))\n",
    "    return out0, out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:18.391459Z",
     "start_time": "2018-11-10T02:29:18.384671Z"
    }
   },
   "outputs": [],
   "source": [
    "def _solve(A,b):\n",
    "    solution = sparse.linalg.spsolve(A.astype(np.float64), b.astype(np.float64))\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:18.624072Z",
     "start_time": "2018-11-10T02:29:18.618511Z"
    }
   },
   "outputs": [],
   "source": [
    "def _diag(x):\n",
    "    return sparse.diags(x.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:18.895935Z",
     "start_time": "2018-11-10T02:29:18.882053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'split:0' shape=(?, 768, 576, 1) dtype=float32>,\n",
       " <tf.Tensor 'split:1' shape=(?, 768, 576, 1) dtype=float32>,\n",
       " <tf.Tensor 'split:2' shape=(?, 768, 576, 1) dtype=float32>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.split(fcn8s_output, [1,1,1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:19.329417Z",
     "start_time": "2018-11-10T02:29:19.323496Z"
    }
   },
   "outputs": [],
   "source": [
    "def matting_laplacians( inputs, prob_bs, prob_fs):\n",
    "    return tf.py_func(matting_laplacians_impl, [inputs, prob_bs, prob_fs], [tf.int32, tf.float64, tf.int64] , stateful=True, name = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:21.402054Z",
     "start_time": "2018-11-10T02:29:21.389089Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_ = tf.get_variable(name = 'lambdal_', initializer=tf.constant(1, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:21.597477Z",
     "start_time": "2018-11-10T02:29:21.589431Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_cpu( prob_f, prob_b, lambda_, x_indices, x_values, x_shapes):\n",
    "    \n",
    "    start_time = time()\n",
    "    \n",
    "    def convert_to_laplacians(x_indices, x_values, x_shapes):\n",
    "        laplacians = list()\n",
    "        for x_indice, x_value, x_shape in zip(x_indices, x_values, x_shapes):\n",
    "            laplacian = sparse.coo_matrix((x_value, (x_indice[:,0], x_indice[:,1])), shape=x_shape )\n",
    "            laplacians.append(laplacian)\n",
    "        return np.asarray(laplacians)\n",
    "    \n",
    "    img_shape = prob_b[0].shape\n",
    "    nm = prob_b[0].shape[0]\n",
    "    lambda_f = float(lambda_)\n",
    "    laplacians = convert_to_laplacians(x_indices, x_values, x_shapes)\n",
    "    cost_time = time() - start_time\n",
    "    log.logger.debug('recontruct laplacian cost: {0}'.format(cost_time))\n",
    "    \n",
    "    ret = []\n",
    "    for b, f, lablacian in zip(prob_b, prob_f, laplacians):\n",
    "        BF_diag = _diag( b + f)\n",
    "        F = f.reshape(-1)\n",
    "        \n",
    "        \n",
    "        D = sparse.csc_matrix(BF_diag * lambda_f + lablacian)\n",
    "        alpha = _solve( D, F*lambda_f)\n",
    "        \n",
    "        \n",
    "        ret.append(alpha.reshape(img_shape))\n",
    "    cost_time = time() - start_time\n",
    "    log.logger.debug('forward cpu cost: {0}'.format(cost_time))\n",
    "    return np.array(ret, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:22.067798Z",
     "start_time": "2018-11-10T02:29:22.053658Z"
    }
   },
   "outputs": [],
   "source": [
    "def backward_cpu(op, gy):\n",
    "    prob_b = op.inputs[0]\n",
    "    prob_f = op.inputs[1]\n",
    "    #laplacians = op.inputs[2]\n",
    "    lambda_ = op.inputs[2]\n",
    "    \n",
    "    x_indices = op.inputs[3]\n",
    "    x_values = op.inputs[4]\n",
    "    x_shapes = op.inputs[5]\n",
    "    \n",
    "    alpha = op.outputs[0]\n",
    "    \n",
    "    def convert_to_laplacians(x_indices, x_values, x_shapes):\n",
    "        laplacians = list()\n",
    "        for x_indice, x_value, x_shape in zip(x_indices, x_values, x_shapes):\n",
    "            laplacian = sparse.coo_matrix((x_value, (x_indice[:,0], x_indice[:,1])), shape=x_shape )\n",
    "            laplacians.append(laplacian)\n",
    "        return np.asarray(laplacians)\n",
    "    laplacians = convert_to_laplacians(x_indices, x_values, x_shapes)\n",
    "    \n",
    "    \n",
    "    mg_shape = prob_b[0].shape\n",
    "    nm = prob_b[0].size\n",
    "    lambda_f = float(lambda_)\n",
    "\n",
    "    ret0, ret1, ret2, ret3 = [], [], [],[]\n",
    "\n",
    "    for b, f, lap, alpha, gy0 in zip(prob_b, prob_f, laps, alphas, gy):\n",
    "        BF_diag = _diag(b+f)\n",
    "        F = f.reshape(-1)\n",
    "        gY = gy0.reshape(-1)\n",
    "\n",
    "        D = sparse.csc_matrix(BF_diag * lambda_f + lap)\n",
    "        D_inv_F_lambda = alpha\n",
    "\n",
    "        gb = _solve(D, -lambda_f * (_diag(D_inv_F_lambda)* gY))\n",
    "        ret0.append(gb.reshape(img_shape))\n",
    "        gf = gb + _solve(D, lambda_f* gY)\n",
    "        ret1.append(gf.reshape(img_shape))\n",
    "        gl = _solve(D, -BF_diag * D_inv_F_lambda)\n",
    "        gl += D_inv_F_lambda / lambda_f\n",
    "        gl = gl.dot(gY)\n",
    "        ret3.append(gl)\n",
    "        \n",
    "    ret = []\n",
    "    ret.append( np.asarray(ret0, dtype=np.float32) )\n",
    "    ret.append(  np.asarray(ret1, dtype=np.float32) )\n",
    "    ret3 = np.sum(ret3).reshape(1)\n",
    "    ret.append( np.asarray(ret3, dtype=np.float32) )\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:24.113730Z",
     "start_time": "2018-11-10T02:29:24.106186Z"
    }
   },
   "outputs": [],
   "source": [
    "def py_func(func, inp, Tout, graph, stateful = True, name = None, grad = None):\n",
    "    rnd_name = 'PyFuncGrad'+ str(np.random.randint(0, 1E+8))\n",
    "    tf.RegisterGradient(rnd_name)(grad)\n",
    "    with graph.gradient_override_map({\"matting_layer\": rnd_name}):\n",
    "        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:24.338582Z",
     "start_time": "2018-11-10T02:29:24.329372Z"
    }
   },
   "outputs": [],
   "source": [
    "prob_fs, prob_bs, prob_us = tf.split(fcn8s_output, [1,1,1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:24.554523Z",
     "start_time": "2018-11-10T02:29:24.546798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(768), Dimension(576), Dimension(1)])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_fs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:24.760139Z",
     "start_time": "2018-11-10T02:29:24.752291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(768), Dimension(576), Dimension(1)])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_bs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:24.949423Z",
     "start_time": "2018-11-10T02:29:24.941472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(768), Dimension(576), Dimension(4)])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:25.191239Z",
     "start_time": "2018-11-10T02:29:25.185914Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:25.873679Z",
     "start_time": "2018-11-10T02:29:25.863552Z"
    }
   },
   "outputs": [],
   "source": [
    "def matting_func( fcn8s_output, inputs, lambda_, name = None):\n",
    "    with tf.name_scope(name, \"matting_layer\", [fcn8s_output]) as name:\n",
    "        prob_fs, prob_bs, prob_us = tf.split(fcn8s_output, [1,1,1], 3)\n",
    "        laplacians = matting_laplacians(inputs, prob_bs, prob_fs)\n",
    "        return py_func(forward_cpu, \n",
    "                       [prob_fs, prob_bs, lambda_, laplacians[0], laplacians[1], laplacians[2]], \n",
    "                       [tf.float32], \n",
    "                       graph = tf.get_default_graph(), \n",
    "                       name = name, \n",
    "                       grad = backward_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:29:28.218018Z",
     "start_time": "2018-11-10T02:29:28.202970Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('matting') as scope:\n",
    "    alpha_ = matting_func(fcn8s_output, inputs, lambda_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:02:15.759962Z",
     "start_time": "2018-11-09T09:02:15.753273Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T01:29:12.851191Z",
     "start_time": "2018-11-11T01:29:12.843565Z"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_schedule(step):\n",
    "    if step <= 5000: return 0.0001\n",
    "    elif 5000 < step <= 7000: return 0.00001\n",
    "    elif 7000 < step <= 8000: return 0.000003\n",
    "    else: return 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T01:29:17.509207Z",
     "start_time": "2018-11-11T01:29:16.513349Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "default hyper params\n",
    "++++++++++++++++++++\n",
    "learning_rate = 1e-4\n",
    "momentum = 0.99\n",
    "weight_decay = 0.005\n",
    "++++++++++++++++++++\n",
    "\"\"\"\n",
    "with tf.name_scope('trimap_pretrain') as scope:\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    trimap_approximation_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=fcn8s_output), \n",
    "        name='trimap_approximation_loss')\n",
    "    trimap_learning_rate = tf.placeholder(dtype=tf.float32, shape=(), name='learning_rate')\n",
    "    trimap_regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    trimap_total_loss = tf.add(trimap_approximation_loss, trimap_regularization_losses, name='trimap_total_loss')\n",
    "    #lr = tf.train.exponential_decay( 1e-2, global_step=global_step, decay_steps=1000,decay_rate=0.05)\n",
    "    #tf.train.exponential_decay( 1e-3, global_step=step, decay_steps=1000,decay_rate=0.05)\n",
    "    trimap_optimizer = tf.train.AdamOptimizer(learning_rate=trimap_learning_rate, name='adam_optimizer')\n",
    "    trimap_train_op = trimap_optimizer.minimize(trimap_total_loss, global_step=global_step, name='trimap_train_op')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T01:29:20.283976Z",
     "start_time": "2018-11-11T01:29:20.241698Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('trimap_predictor'):\n",
    "    softmax_output = tf.nn.softmax(fcn8s_output, name='trimap_softmax_output')\n",
    "    predictions_argmax = tf.argmax(softmax_output, axis=-1, name='predictions_argmax', output_type=tf.int64)\n",
    "    labels_argmax = tf.argmax(labels, axis=-1, name='trimap_labels_argmax', output_type=tf.int64)\n",
    "    acc_value, acc_update_op = tf.metrics.accuracy(labels=labels_argmax, predictions= predictions_argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:30:06.431457Z",
     "start_time": "2018-11-10T02:30:06.407185Z"
    }
   },
   "outputs": [],
   "source": [
    "bg = BatchGenerator(img_dirs = img_dir,\n",
    "                    img_file_format = 'png',\n",
    "                    prior_info_dir_list = priorInfo_dir_lsit,\n",
    "                    ground_truth_dir = trimap_dir,\n",
    "                    num_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:30:11.763688Z",
     "start_time": "2018-11-10T02:30:06.759022Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 23.85it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_inp, valid_gt = bg.genValidata(valid_size=100, \n",
    "                                       convert_labels_to_one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T02:30:11.769371Z",
     "start_time": "2018-11-10T02:30:11.766336Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-11T01:35:03.145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 00000 \t Learning_rate 0.0001 \t Loss 1.0818300247192383 \t pixel_match 0.5301993815104167\n",
      "Step 00020 \t Learning_rate 0.0001 \t Loss 1.038285493850708 \t pixel_match 0.4139742702907986\n",
      "Step 00040 \t Learning_rate 0.0001 \t Loss 0.7472904324531555 \t pixel_match 0.6259330692997684\n",
      "Step 00060 \t Learning_rate 0.0001 \t Loss 0.5512394905090332 \t pixel_match 0.7563595015914353\n",
      "Step 00080 \t Learning_rate 0.0001 \t Loss 0.6423148512840271 \t pixel_match 0.7563888662832753\n",
      "Step 00100 \t Learning_rate 0.0001 \t Loss 0.4626580774784088 \t pixel_match 0.8313560881438078\n",
      "Step 00120 \t Learning_rate 0.0001 \t Loss 0.41962334513664246 \t pixel_match 0.8507206443504053\n",
      "Step 00140 \t Learning_rate 0.0001 \t Loss 0.3935807943344116 \t pixel_match 0.8524341046368634\n",
      "Step 00160 \t Learning_rate 0.0001 \t Loss 0.3900660276412964 \t pixel_match 0.8581215639467592\n",
      "Step 00180 \t Learning_rate 0.0001 \t Loss 0.4077347218990326 \t pixel_match 0.8585902913411458\n",
      "Step 00200 \t Learning_rate 0.0001 \t Loss 0.3611046075820923 \t pixel_match 0.866210779260706\n",
      "Step 00220 \t Learning_rate 0.0001 \t Loss 0.3411179482936859 \t pixel_match 0.8743240469473383\n",
      "Step 00240 \t Learning_rate 0.0001 \t Loss 0.35610446333885193 \t pixel_match 0.8646643970630788\n",
      "Step 00260 \t Learning_rate 0.0001 \t Loss 0.3128010034561157 \t pixel_match 0.8831334318938079\n",
      "Step 00280 \t Learning_rate 0.0001 \t Loss 0.3192676305770874 \t pixel_match 0.8768786168981481\n",
      "Step 00300 \t Learning_rate 0.0001 \t Loss 0.32569625973701477 \t pixel_match 0.87890869140625\n",
      "Step 00320 \t Learning_rate 0.0001 \t Loss 0.33347105979919434 \t pixel_match 0.8759801115813081\n",
      "Step 00340 \t Learning_rate 0.0001 \t Loss 0.30056846141815186 \t pixel_match 0.8878247522424769\n",
      "Shape incorrect, c_inp shape:(1, 768, 576, 4)  c_gt shape:(1, 768, 576, 3)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Step 00360 \t Learning_rate 0.0001 \t Loss 0.2982582449913025 \t pixel_match 0.8842147940176504\n",
      "Step 00380 \t Learning_rate 0.0001 \t Loss 0.3337302803993225 \t pixel_match 0.8721684208622686\n",
      "Step 00400 \t Learning_rate 0.0001 \t Loss 0.2807199954986572 \t pixel_match 0.8867540147569444\n",
      "Step 00420 \t Learning_rate 0.0001 \t Loss 0.271186500787735 \t pixel_match 0.8896524386935764\n",
      "Step 00440 \t Learning_rate 0.0001 \t Loss 0.25975921750068665 \t pixel_match 0.8912792742693866\n",
      "Step 00460 \t Learning_rate 0.0001 \t Loss 0.3336334526538849 \t pixel_match 0.8702163357204862\n",
      "Step 00480 \t Learning_rate 0.0001 \t Loss 0.3023051917552948 \t pixel_match 0.8825391981336804\n",
      "Step 00500 \t Learning_rate 0.0001 \t Loss 0.26401326060295105 \t pixel_match 0.8925498905888309\n",
      "Step 00520 \t Learning_rate 0.0001 \t Loss 0.288321316242218 \t pixel_match 0.8890279134114585\n",
      "Step 00540 \t Learning_rate 0.0001 \t Loss 0.26889804005622864 \t pixel_match 0.89334716796875\n",
      "Step 00560 \t Learning_rate 0.0001 \t Loss 0.2639334797859192 \t pixel_match 0.8935033501519097\n",
      "Step 00580 \t Learning_rate 0.0001 \t Loss 0.2682114541530609 \t pixel_match 0.8934672489872685\n",
      "Step 00600 \t Learning_rate 0.0001 \t Loss 0.24414795637130737 \t pixel_match 0.8961949553313079\n",
      "Step 00620 \t Learning_rate 0.0001 \t Loss 0.2655807435512543 \t pixel_match 0.896435162579572\n",
      "Step 00640 \t Learning_rate 0.0001 \t Loss 0.24577507376670837 \t pixel_match 0.8977608235677085\n",
      "Step 00660 \t Learning_rate 0.0001 \t Loss 0.24839258193969727 \t pixel_match 0.8943606002242477\n",
      "Step 00680 \t Learning_rate 0.0001 \t Loss 0.3322429656982422 \t pixel_match 0.8681448703342012\n",
      "Step 00700 \t Learning_rate 0.0001 \t Loss 0.3118111491203308 \t pixel_match 0.8830298077618635\n",
      "Shape incorrect, c_inp shape:(1, 768, 576, 4)  c_gt shape:(1, 768, 576, 3)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Shape incorrect, c_inp shape:(0,)  c_gt shape:(0,)\n",
      "Step 00720 \t Learning_rate 0.0001 \t Loss 0.293928325176239 \t pixel_match 0.8867563657407408\n",
      "Step 00740 \t Learning_rate 0.0001 \t Loss 0.26814183592796326 \t pixel_match 0.89509765625\n",
      "Step 00760 \t Learning_rate 0.0001 \t Loss 0.25513842701911926 \t pixel_match 0.89382568359375\n",
      "Step 00780 \t Learning_rate 0.0001 \t Loss 0.24464505910873413 \t pixel_match 0.8975724283854166\n",
      "Step 00800 \t Learning_rate 0.0001 \t Loss 0.25244587659835815 \t pixel_match 0.8979912199797454\n",
      "Step 00820 \t Learning_rate 0.0001 \t Loss 0.24504001438617706 \t pixel_match 0.89889562536169\n",
      "Step 00840 \t Learning_rate 0.0001 \t Loss 0.2676612436771393 \t pixel_match 0.8974755181206596\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "model_path = './TrimapPretrainedModel/'\n",
    "model_name = 'fcn8smatting_pretrained.cpkt'\n",
    "max_iter = 10000\n",
    "pixel_match = -1\n",
    "with tf.Session() as sess:\n",
    "    batchG = bg.generate(batch_size = batch_size,\n",
    "                       convert_labels_to_one_hot = True)\n",
    "    init = tf.group([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "    sess.run(init)\n",
    "    cnt = 0\n",
    "    while pixel_match < 0.97:\n",
    "        c_inp, c_gt = next(batchG)\n",
    "        step = sess.run(global_step)\n",
    "        lr = learning_rate_schedule(step)\n",
    "        if not (( c_inp.shape == ( batch_size, 768, 576, 4) ) and (c_gt.shape == ( batch_size, 768, 576, 3)) ):\n",
    "            print('Shape incorrect, c_inp shape:{0}  c_gt shape:{1}'.format(c_inp.shape, c_gt.shape))\n",
    "            continue\n",
    "        sess.run(trimap_train_op, feed_dict={inputs: c_inp, labels: c_gt, trimap_learning_rate:lr, keeprob: 0.5, l2_regularization_rate:0.0})\n",
    "        if cnt%20 == 0:\n",
    "            t_losses = []\n",
    "            t_acc = []\n",
    "            t_pm = []\n",
    "            for i in range(valid_inp.shape[0]):\n",
    "                slice_vinp = np.expand_dims(valid_inp[i,:,:,:],0)\n",
    "                slice_vgt = np.expand_dims( valid_gt[i,:,:,:],0)\n",
    "                loss, acc_v, pd_argmax, lb_argmax = sess.run([trimap_total_loss,acc_value, predictions_argmax, labels_argmax], feed_dict={inputs: slice_vinp, labels: slice_vgt, keeprob: 1.0, l2_regularization_rate:0.0})\n",
    "                t_losses.append(np.mean(loss))\n",
    "                t_acc.append(acc_v)\n",
    "                t_pm.append(np.sum(pd_argmax == lb_argmax)/pd_argmax.size)\n",
    "            #t_losses, acc_v, pd_argmax, lb_argmax = sess.run([total_loss,acc_value, predictions_argmax, labels_argmax], feed_dict={inputs: valid_inp, labels: valid_gt, keeprob: 1.0,l2_regularization_rate:0.0})\n",
    "            #acc_v = sess.run(acc_value)\n",
    "            pm = np.mean(t_pm)\n",
    "            if pm > pixel_match:\n",
    "                pixel_match = pm\n",
    "            print('Step {0:00005} \\t Learning_rate {1} \\t Loss {2} \\t pixel_match {3}'.format(step, lr,np.mean(t_losses), pm))\n",
    "        cnt += 1\n",
    "        if (cnt%1000 == 0) or (pm > 0.9):\n",
    "            saver.save(sess, model_path + 'fcn8smatting_pretrained_{0}_{1}.ckpt'.format(cnt, pm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T01:28:23.329416Z",
     "start_time": "2018-11-10T07:49:07.664Z"
    }
   },
   "outputs": [],
   "source": [
    "print('{0} loops with batch_size {1} and loss is {2}, we obtained pixel_match: {3}'.format(cnt, batch_size, np.mean(t_losses), pm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
